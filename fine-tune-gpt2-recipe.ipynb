{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c798fdd",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-tune GPT-2 (Decoder-only) for Recipe Generation (Kaggle-ready)\n",
    "\n",
    "This notebook fine-tunes GPT-2 on the provided Kaggle recipe dataset using LoRA (PEFT) for parameter-efficient training.\n",
    "Follow instructions and run **Run All** in Kaggle. It samples 20% of the dataset (overall) for fine-tuning, splits into train/validation/test, and uses checkpoints to resume training if interrupted.\n",
    "\n",
    "**Notes**\n",
    "- The notebook installs required libraries (transformers, datasets, accelerate, peft, bitsandbytes if desired).\n",
    "- Uses mixed precision (fp16) by default; contains a fallback branch if 4-bit quantization is available in the environment.\n",
    "- Checkpoints are saved in `checkpoints/`.\n",
    "- Final model `model.pkl` (state_dict + tokenizer files saved separately) will be saved to `/kaggle/working/` and `/kaggle/working/checkpoints/`.\n",
    "- Do **not** run this on CPU-only without adjusting batch sizes; Kaggle GPU is recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae39407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:00:12.847322Z",
     "iopub.status.busy": "2025-10-29T12:00:12.847050Z",
     "iopub.status.idle": "2025-10-29T12:01:57.680962Z",
     "shell.execute_reply": "2025-10-29T12:01:57.680019Z",
     "shell.execute_reply.started": "2025-10-29T12:00:12.847303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q transformers datasets accelerate peft[safe] evaluate sentencepiece\n",
    "!pip install -q git+https://github.com/huggingface/peft.git@main\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec87f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:01:57.682674Z",
     "iopub.status.busy": "2025-10-29T12:01:57.682436Z",
     "iopub.status.idle": "2025-10-29T12:02:36.554974Z",
     "shell.execute_reply": "2025-10-29T12:02:36.554187Z",
     "shell.execute_reply.started": "2025-10-29T12:01:57.682653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, math, random, glob, pickle, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Using {num_gpus} GPU(s)\")\n",
    "\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, set_seed, logging\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "logging.set_verbosity_info()\n",
    "set_seed(42)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a86c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:02:36.556319Z",
     "iopub.status.busy": "2025-10-29T12:02:36.555773Z",
     "iopub.status.idle": "2025-10-29T12:02:36.572597Z",
     "shell.execute_reply": "2025-10-29T12:02:36.571793Z",
     "shell.execute_reply.started": "2025-10-29T12:02:36.556301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_root = '/kaggle/input'\n",
    "candidates = []\n",
    "for root, dirs, files in os.walk(input_root):\n",
    "    for f in files:\n",
    "        if f.endswith(('.csv','.json','.txt')):\n",
    "            candidates.append(os.path.join(root,f))\n",
    "candidates[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2992c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:02:36.574746Z",
     "iopub.status.busy": "2025-10-29T12:02:36.574526Z",
     "iopub.status.idle": "2025-10-29T12:03:24.563153Z",
     "shell.execute_reply": "2025-10-29T12:03:24.562431Z",
     "shell.execute_reply.started": "2025-10-29T12:02:36.574729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Adjust this path if you know the exact file inside /kaggle/input\n",
    "files = [p for p in candidates if p.endswith('.csv') or p.endswith('.json')]\n",
    "if len(files)==0:\n",
    "    raise FileNotFoundError('No dataset files found under /kaggle/input. Upload the Kaggle dataset to the notebook.')\n",
    "data_path = files[0]\n",
    "print('Using', data_path)\n",
    "if data_path.endswith('.csv'):\n",
    "    df = pd.read_csv(data_path)\n",
    "else:\n",
    "    df = pd.read_json(data_path, lines=True)\n",
    "print('Rows:', len(df))\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1adc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:03:24.564234Z",
     "iopub.status.busy": "2025-10-29T12:03:24.563971Z",
     "iopub.status.idle": "2025-10-29T12:03:24.570738Z",
     "shell.execute_reply": "2025-10-29T12:03:24.569802Z",
     "shell.execute_reply.started": "2025-10-29T12:03:24.564200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Attempt to detect common fields for recipes\n",
    "possible_title = [c for c in df.columns if 'title' in c.lower() or 'name' in c.lower()]\n",
    "possible_ingredients = [c for c in df.columns if 'ingredient' in c.lower() or 'ingredients' in c.lower()]\n",
    "possible_instructions = [c for c in df.columns if 'instruction' in c.lower() or 'directions' in c.lower() or 'steps' in c.lower()]\n",
    "\n",
    "print('title candidates:', possible_title)\n",
    "print('ingredients candidates:', possible_ingredients)\n",
    "print('instructions candidates:', possible_instructions)\n",
    "\n",
    "title_col = possible_title[0] if possible_title else None\n",
    "ing_col = possible_ingredients[0] if possible_ingredients else None\n",
    "inst_col = possible_instructions[0] if possible_instructions else None\n",
    "\n",
    "if not (ing_col or title_col) or not inst_col:\n",
    "    # fallback: try columns by manual inspection (first 6 columns)\n",
    "    print('Automatic detection failed. Showing first 6 columns for manual pick.')\n",
    "    display(df.iloc[:5,:6])\n",
    "else:\n",
    "    print('Detected columns:', title_col, ing_col, inst_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a68b35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:03:24.571757Z",
     "iopub.status.busy": "2025-10-29T12:03:24.571564Z",
     "iopub.status.idle": "2025-10-29T12:05:21.919056Z",
     "shell.execute_reply": "2025-10-29T12:05:21.918400Z",
     "shell.execute_reply.started": "2025-10-29T12:03:24.571742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a text prompt->target format. If some columns are missing, we craft reasonable fallbacks.\n",
    "def row_to_pair(r):\n",
    "    title = r[title_col] if title_col in r and pd.notnull(r[title_col]) else ''\n",
    "    ings = r[ing_col] if ing_col in r and pd.notnull(r[ing_col]) else ''\n",
    "    inst = r[inst_col] if inst_col in r and pd.notnull(r[inst_col]) else ''\n",
    "    if isinstance(ings, (list, tuple)):\n",
    "        ings = ', '.join(ings)\n",
    "    return title, ings, inst\n",
    "\n",
    "pairs = []\n",
    "for _, r in df.iterrows():\n",
    "    title, ings, inst = row_to_pair(r)\n",
    "    if not inst or (not ings and not title):\n",
    "        continue\n",
    "    prompt = ''\n",
    "    if ings:\n",
    "        prompt += 'Ingredients: ' + str(ings).strip() + '\\n'\n",
    "    if title:\n",
    "        prompt += 'Title: ' + str(title).strip() + '\\n'\n",
    "    prompt += 'Recipe:\\n'\n",
    "    target = str(inst).strip()\n",
    "    text = prompt + target\n",
    "    pairs.append({'text': text})\n",
    "\n",
    "if len(pairs)==0:\n",
    "    raise ValueError('No usable recipe rows found. Please inspect the dataset and set correct columns.')\n",
    "\n",
    "df_pairs = pd.DataFrame(pairs)\n",
    "sample_frac = 0.20\n",
    "df_sample = df_pairs.sample(frac=sample_frac, random_state=42).reset_index(drop=True)\n",
    "print('Original rows:', len(df_pairs), 'Sampled (20%):', len(df_sample))\n",
    "\n",
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "test_frac = 0.1\n",
    "n = len(df_sample)\n",
    "n_train = int(n * train_frac)\n",
    "n_val = int(n * val_frac)\n",
    "train_df = df_sample.iloc[:n_train]\n",
    "val_df = df_sample.iloc[n_train:n_train+n_val]\n",
    "test_df = df_sample.iloc[n_train+n_val:]\n",
    "print('Train/Val/Test sizes:', len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(val_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff8045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:05:21.920283Z",
     "iopub.status.busy": "2025-10-29T12:05:21.920016Z",
     "iopub.status.idle": "2025-10-29T12:06:41.367043Z",
     "shell.execute_reply": "2025-10-29T12:06:41.366507Z",
     "shell.execute_reply.started": "2025-10-29T12:05:21.920255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.add_special_tokens({'pad_token':'<|pad|>'})\n",
    "\n",
    "def preprocess(examples):\n",
    "    outputs = tokenizer(examples['text'], truncation=True, max_length=512)\n",
    "    return outputs\n",
    "\n",
    "tokenized = dataset_dict.map(preprocess, batched=True, remove_columns=['text'])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# ‚úÖ Reduce dataset size to 15% for faster training\n",
    "train_dataset = tokenized[\"train\"].shuffle(seed=42).select(range(int(0.15 * len(tokenized[\"train\"]))))\n",
    "eval_dataset = tokenized[\"validation\"].shuffle(seed=42).select(range(int(0.15 * len(tokenized[\"validation\"]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72fcdd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:06:41.367945Z",
     "iopub.status.busy": "2025-10-29T12:06:41.367739Z",
     "iopub.status.idle": "2025-10-29T12:06:51.236811Z",
     "shell.execute_reply": "2025-10-29T12:06:51.235925Z",
     "shell.execute_reply.started": "2025-10-29T12:06:41.367929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "device_map = 'auto' if torch.cuda.is_available() else None\n",
    "use_4bit = False\n",
    "try:\n",
    "    if 'bnb' in sys.modules:\n",
    "        from transformers import AutoConfig\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map, load_in_4bit=False)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map)\n",
    "except Exception as e:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print('Model loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d6bbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:06:51.237985Z",
     "iopub.status.busy": "2025-10-29T12:06:51.237720Z",
     "iopub.status.idle": "2025-10-29T12:06:51.274926Z",
     "shell.execute_reply": "2025-10-29T12:06:51.274225Z",
     "shell.execute_reply.started": "2025-10-29T12:06:51.237967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "target_modules = ['c_attn','q_proj','v_proj','k_proj']\n",
    "\n",
    "try:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "print('LoRA applied. Parameter count (trainable):', sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6953e8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:06:51.277581Z",
     "iopub.status.busy": "2025-10-29T12:06:51.277053Z",
     "iopub.status.idle": "2025-10-29T12:49:13.575997Z",
     "shell.execute_reply": "2025-10-29T12:49:13.575235Z",
     "shell.execute_reply.started": "2025-10-29T12:06:51.277562Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/gpt2-lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\",\n",
    "    ddp_find_unused_parameters=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(training_args.output_dir)\n",
    "tokenizer.save_pretrained(training_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104e10f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:49:13.577261Z",
     "iopub.status.busy": "2025-10-29T12:49:13.576967Z",
     "iopub.status.idle": "2025-10-29T12:49:16.095012Z",
     "shell.execute_reply": "2025-10-29T12:49:16.094173Z",
     "shell.execute_reply.started": "2025-10-29T12:49:13.577233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "save_path = '/kaggle/working/model.pkl'\n",
    "to_save = {}\n",
    "to_save['state_dict'] = model.state_dict()\n",
    "to_save['config'] = model.config.to_dict()\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(to_save, f)\n",
    "print('Saved model.pkl to', save_path)\n",
    "\n",
    "# Also save tokenizer and peft adapter\n",
    "tokenizer.save_pretrained('/kaggle/working/tokenizer')\n",
    "model.save_pretrained('/kaggle/working/peft_model')\n",
    "print('Saved tokenizer and peft model to /kaggle/working/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f7b1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T12:49:16.096061Z",
     "iopub.status.busy": "2025-10-29T12:49:16.095816Z",
     "iopub.status.idle": "2025-10-29T12:49:16.445049Z",
     "shell.execute_reply": "2025-10-29T12:49:16.443939Z",
     "shell.execute_reply.started": "2025-10-29T12:49:16.096043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "gen = pipeline('text-generation', model=output_dir, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "examples = [\n",
    "    'Ingredients: egg, flour, sugar\\nTitle: Simple Pancakes\\nRecipe:\\n',\n",
    "    'Ingredients: chicken, garlic, salt, pepper\\nTitle: Garlic Chicken\\nRecipe:\\n',\n",
    "]\n",
    "for ex in examples:\n",
    "    o = gen(ex, max_length=300, num_return_sequences=1)[0]['generated_text']\n",
    "    print('---PROMPT---\\n', ex)\n",
    "    print('---GENERATED---\\n', o.replace(ex,''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382dc427",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-29T12:49:16.445532Z",
     "iopub.status.idle": "2025-10-29T12:49:16.445761Z",
     "shell.execute_reply": "2025-10-29T12:49:16.445653Z",
     "shell.execute_reply.started": "2025-10-29T12:49:16.445643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "metric_bleu = evaluate.load('bleu')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def compute_bleu(preds, refs):\n",
    "    preds_tok = [word_tokenize(p) for p in preds]\n",
    "    refs_tok = [[word_tokenize(r)] for r in refs]\n",
    "    return metric_bleu.compute(predictions=preds_tok, references=refs_tok)\n",
    "\n",
    "# Simple eval on test set (generate using prompts from test set)\n",
    "test_texts = [x['text'] for x in dataset_dict['test']]\n",
    "gen_texts = []\n",
    "for t in test_texts[:20]:\n",
    "    prompt = t.split('Recipe:\\n')[0] + 'Recipe:\\n'\n",
    "    out = gen(prompt, max_length=512, num_return_sequences=1)[0]['generated_text']\n",
    "    gen_texts.append(out.replace(prompt,''))\n",
    "refs = [t.split('Recipe:\\n',1)[1] for t in test_texts[:20]]\n",
    "bleu_res = compute_bleu(gen_texts, refs)\n",
    "print('BLEU (sample):', bleu_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c969d3e1",
   "metadata": {},
   "source": [
    "# **MODEL EVALUATION**\n",
    "\n",
    "This section is designed to evaluate the fine-tuned GPT-2 model independently. It loads the saved model from disk and performs comprehensive evaluation without depending on the training cells above.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ Independent model loading from saved files\n",
    "- ‚úÖ Comprehensive evaluation metrics (BLEU, ROUGE, Perplexity)\n",
    "- ‚úÖ Sample generation with various prompts\n",
    "- ‚úÖ Performance analysis and visualization\n",
    "- ‚úÖ Recipe quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß EVALUATION SETUP - Independent Model Loading (CPU Optimized)\n",
    "# This cell loads the fine-tuned model independently for evaluation on local CPU\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Force CPU usage for local environment - no CUDA/TPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"üñ•Ô∏è Using device: {device} (forced CPU for local evaluation)\")\n",
    "print(\"‚ö†Ô∏è Note: CPU evaluation will be slower but more compatible\")\n",
    "\n",
    "# Load required libraries for evaluation\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer, \n",
    "        AutoModelForCausalLM, \n",
    "        GenerationConfig\n",
    "    )\n",
    "    from peft import PeftModel\n",
    "    print(\"‚úÖ Core libraries loaded successfully\")\n",
    "    \n",
    "    # Try to load evaluation metrics (optional for minimal setup)\n",
    "    try:\n",
    "        import evaluate\n",
    "        print(\"‚úÖ Evaluation metrics available\")\n",
    "        METRICS_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Evaluation metrics not available (install with: pip install evaluate)\")\n",
    "        METRICS_AVAILABLE = False\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error loading libraries: {e}\")\n",
    "    print(\"Please install missing packages: pip install transformers peft\")\n",
    "\n",
    "# Define paths\n",
    "BASE_PATH = Path(\".\")\n",
    "MODEL_PATH = BASE_PATH / \"models\" / \"model.pkl\"\n",
    "TOKENIZER_PATH = BASE_PATH / \"tokenizer\"\n",
    "PEFT_MODEL_PATH = BASE_PATH / \"peft_model\"\n",
    "TEST_DATA_PATH = BASE_PATH / \"models\" / \"test_data.pkl\"\n",
    "\n",
    "print(f\"\\nüìÅ File Check:\")\n",
    "print(f\"   ‚Ä¢ Model path: {MODEL_PATH}\")\n",
    "print(f\"   ‚Ä¢ Tokenizer path: {TOKENIZER_PATH}\")\n",
    "print(f\"   ‚Ä¢ PEFT model path: {PEFT_MODEL_PATH}\")\n",
    "print(f\"   ‚Ä¢ Test data path: {TEST_DATA_PATH}\")\n",
    "\n",
    "# Check if files exist\n",
    "missing_files = []\n",
    "for path_name, path in [(\"Model\", MODEL_PATH), (\"Tokenizer\", TOKENIZER_PATH), \n",
    "                       (\"PEFT Model\", PEFT_MODEL_PATH), (\"Test Data\", TEST_DATA_PATH)]:\n",
    "    if not path.exists():\n",
    "        missing_files.append(f\"{path_name}: {path}\")\n",
    "        print(f\"‚ùå Missing: {path}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found: {path}\")\n",
    "\n",
    "# Check specifically for PEFT adapter weights\n",
    "if PEFT_MODEL_PATH.exists():\n",
    "    adapter_files = list(PEFT_MODEL_PATH.glob(\"adapter_model.*\"))\n",
    "    if adapter_files:\n",
    "        print(f\"‚úÖ PEFT adapter weights found: {[f.name for f in adapter_files]}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è PEFT directory exists but no adapter weights found\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: Some files are missing. Evaluation will be limited to available files.\")\n",
    "    print(\"üîß The evaluation will adapt to use whatever files are available.\")\n",
    "else:\n",
    "    print(f\"\\nüéâ All required files found! Ready for evaluation.\")\n",
    "\n",
    "EVALUATION_READY = True  # Always ready, just adapt to available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ LOAD FINE-TUNED MODEL FOR EVALUATION (CPU Optimized)\n",
    "# This cell loads the fine-tuned GPT-2 model with multiple fallback strategies\n",
    "\n",
    "def load_finetuned_model():\n",
    "    \"\"\"Load the fine-tuned model with CPU optimization and fallback strategies\"\"\"\n",
    "    \n",
    "    # Strategy 1: Try PEFT model if available\n",
    "    if TOKENIZER_PATH.exists() and PEFT_MODEL_PATH.exists():\n",
    "        adapter_files = list(PEFT_MODEL_PATH.glob(\"adapter_model.*\"))\n",
    "        if adapter_files:\n",
    "            try:\n",
    "                print(\"üîÑ Loading PEFT model (LoRA adapters)...\")\n",
    "                \n",
    "                # Load tokenizer\n",
    "                tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "                if tokenizer.pad_token is None:\n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                print(f\"‚úÖ Tokenizer loaded (vocab size: {len(tokenizer)})\")\n",
    "                \n",
    "                # Load base model on CPU\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.float32)\n",
    "                base_model.resize_token_embeddings(len(tokenizer))\n",
    "                \n",
    "                # Load PEFT adapters\n",
    "                model = PeftModel.from_pretrained(base_model, PEFT_MODEL_PATH)\n",
    "                model = model.to(device)\n",
    "                model.eval()\n",
    "                \n",
    "                print(f\"‚úÖ PEFT model loaded successfully on {device}!\")\n",
    "                \n",
    "                # Count parameters\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                print(f\"üìà Total parameters: {total_params:,}\")\n",
    "                print(f\"üìà Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "                \n",
    "                return model, tokenizer, \"PEFT\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è PEFT loading failed: {e}\")\n",
    "                print(\"üîÑ Falling back to pickle model...\")\n",
    "    \n",
    "    # Strategy 2: Try model.pkl with tokenizer\n",
    "    if MODEL_PATH.exists() and TOKENIZER_PATH.exists():\n",
    "        try:\n",
    "            print(\"üîÑ Loading from model.pkl with custom tokenizer...\")\n",
    "            \n",
    "            # Load tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Load model from pickle with CPU mapping and PyTorch 2.6 compatibility\n",
    "            model_data = None\n",
    "            try:\n",
    "                # Method 1: Direct torch.load with weights_only=False\n",
    "                model_data = torch.load(MODEL_PATH, map_location='cpu', weights_only=False)\n",
    "                print(\"‚úÖ Loaded with torch.load (weights_only=False)\")\n",
    "            except Exception as e1:\n",
    "                print(f\"‚ö†Ô∏è torch.load failed: {str(e1)[:100]}...\")\n",
    "                try:\n",
    "                    # Method 2: Use pickle directly\n",
    "                    import pickle\n",
    "                    with open(MODEL_PATH, 'rb') as f:\n",
    "                        model_data = pickle.load(f)\n",
    "                    print(\"‚úÖ Loaded with pickle.load\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"‚ö†Ô∏è pickle.load failed: {str(e2)[:100]}...\")\n",
    "                    raise e2\n",
    "            \n",
    "            # Load base model\n",
    "            model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.float32)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "            \n",
    "            # Apply state dict if available\n",
    "            if 'state_dict' in model_data:\n",
    "                model.load_state_dict(model_data['state_dict'], strict=False)\n",
    "                print(\"‚úÖ State dict loaded from pickle\")\n",
    "            \n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            print(f\"‚úÖ Model loaded from pickle with custom tokenizer on {device}!\")\n",
    "            return model, tokenizer, \"Pickle+Tokenizer\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Pickle+tokenizer loading failed: {e}\")\n",
    "    \n",
    "    # Strategy 3: Try model.pkl with default tokenizer\n",
    "    if MODEL_PATH.exists():\n",
    "        try:\n",
    "            print(\"üîÑ Loading from model.pkl with default GPT-2 tokenizer...\")\n",
    "            \n",
    "            # Use default tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Load model from pickle with PyTorch 2.6 compatibility\n",
    "            model_data = None\n",
    "            try:\n",
    "                # Method 1: Direct torch.load with weights_only=False\n",
    "                model_data = torch.load(MODEL_PATH, map_location='cpu', weights_only=False)\n",
    "                print(\"‚úÖ Loaded with torch.load (weights_only=False)\")\n",
    "            except Exception as e1:\n",
    "                print(f\"‚ö†Ô∏è torch.load failed: {str(e1)[:100]}...\")\n",
    "                try:\n",
    "                    # Method 2: Use pickle directly\n",
    "                    import pickle\n",
    "                    with open(MODEL_PATH, 'rb') as f:\n",
    "                        model_data = pickle.load(f)\n",
    "                    print(\"‚úÖ Loaded with pickle.load\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"‚ö†Ô∏è pickle.load failed: {str(e2)[:100]}...\")\n",
    "                    raise e2\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.float32)\n",
    "            \n",
    "            if 'state_dict' in model_data:\n",
    "                try:\n",
    "                    model.load_state_dict(model_data['state_dict'], strict=False)\n",
    "                    print(\"‚úÖ State dict loaded from pickle (non-strict)\")\n",
    "                except Exception as load_error:\n",
    "                    print(f\"‚ö†Ô∏è State dict loading failed: {load_error}\")\n",
    "                    print(\"Using base GPT-2 model instead\")\n",
    "            \n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            print(f\"‚úÖ Model loaded from pickle with default tokenizer on {device}!\")\n",
    "            return model, tokenizer, \"Pickle+Default\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Pickle loading failed: {e}\")\n",
    "    \n",
    "    # Strategy 4: Fallback to base GPT-2\n",
    "    try:\n",
    "        print(\"üîÑ Using base GPT-2 model (not fine-tuned)...\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.float32)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"‚ö†Ô∏è Using base GPT-2 model on {device} (not fine-tuned)\")\n",
    "        return model, tokenizer, \"Base\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå All loading strategies failed: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Load the model\n",
    "eval_model, eval_tokenizer, model_type = load_finetuned_model()\n",
    "MODEL_LOADED = eval_model is not None and eval_tokenizer is not None\n",
    "\n",
    "if MODEL_LOADED:\n",
    "    print(f\"\\nüéâ Model successfully loaded for evaluation! (Type: {model_type})\")\n",
    "    if model_type == \"Base\":\n",
    "        print(\"‚ö†Ô∏è Note: Using base GPT-2. Results may not reflect fine-tuning quality.\")\n",
    "    print(\"üñ•Ô∏è Ready for CPU-based evaluation (generation will be slower)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to load any model. Please check file availability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814eae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä LOAD TEST DATA FOR EVALUATION\n",
    "# Load test data independently for evaluation metrics\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"Load test data for evaluation\"\"\"\n",
    "    try:\n",
    "        if TEST_DATA_PATH.exists():\n",
    "            print(\"üîÑ Loading test data from pickle...\")\n",
    "            with open(TEST_DATA_PATH, 'rb') as f:\n",
    "                test_data = pickle.load(f)\n",
    "            print(f\"‚úÖ Test data loaded: {len(test_data)} samples\")\n",
    "            return test_data\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Test data pickle not found. Trying CSV...\")\n",
    "            csv_path = BASE_PATH / \"models\" / \"test_data.csv\"\n",
    "            if csv_path.exists():\n",
    "                test_df = pd.read_csv(csv_path)\n",
    "                print(f\"‚úÖ Test data loaded from CSV: {len(test_df)} samples\")\n",
    "                return test_df.to_dict('records')\n",
    "            else:\n",
    "                print(\"‚ùå No test data found.\")\n",
    "                return None\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading test data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load test data\n",
    "test_data = load_test_data()\n",
    "TEST_DATA_LOADED = test_data is not None\n",
    "\n",
    "if TEST_DATA_LOADED:\n",
    "    print(f\"üìã Test data summary:\")\n",
    "    print(f\"   ‚Ä¢ Total samples: {len(test_data)}\")\n",
    "    if isinstance(test_data, list) and len(test_data) > 0:\n",
    "        print(f\"   ‚Ä¢ Sample keys: {list(test_data[0].keys())}\")\n",
    "        if 'text' in test_data[0]:\n",
    "            avg_length = np.mean([len(sample['text']) for sample in test_data])\n",
    "            print(f\"   ‚Ä¢ Average text length: {avg_length:.1f} characters\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Test data not available. Will create sample prompts for evaluation.\")\n",
    "    \n",
    "# Create sample evaluation prompts if no test data\n",
    "SAMPLE_PROMPTS = [\n",
    "    \"Ingredients: eggs, flour, sugar, butter, vanilla\\nTitle: Classic Vanilla Cake\\nRecipe:\\n\",\n",
    "    \"Ingredients: chicken breast, garlic, olive oil, salt, pepper\\nTitle: Garlic Chicken\\nRecipe:\\n\",\n",
    "    \"Ingredients: pasta, tomatoes, basil, parmesan, olive oil\\nTitle: Pasta Marinara\\nRecipe:\\n\",\n",
    "    \"Ingredients: salmon, lemon, dill, butter, salt\\nTitle: Lemon Dill Salmon\\nRecipe:\\n\",\n",
    "    \"Ingredients: rice, vegetables, soy sauce, ginger, garlic\\nTitle: Vegetable Fried Rice\\nRecipe:\\n\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüìù Sample prompts prepared: {len(SAMPLE_PROMPTS)} prompts ready for generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fa713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ RECIPE GENERATION FUNCTION (CPU Optimized)\n",
    "# Create a lightweight recipe generation function optimized for CPU\n",
    "\n",
    "def generate_recipe_cpu(model, tokenizer, prompt, max_length=200, temperature=0.8, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate recipe using the model - optimized for CPU with shorter outputs\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded model\n",
    "        tokenizer: Tokenizer\n",
    "        prompt: Input prompt (ingredients + title)\n",
    "        max_length: Maximum generation length (reduced for CPU)\n",
    "        temperature: Sampling temperature\n",
    "        top_p: Nucleus sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        Generated recipe text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Generating recipe (max_length={max_length})...\")\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Generate with CPU-optimized settings\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_length=inputs.shape[1] + max_length,  # Add to input length\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,\n",
    "                length_penalty=1.0,\n",
    "                no_repeat_ngram_size=2,  # Prevent repetition\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract only the generated part (after the prompt)\n",
    "        recipe_text = full_text[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(recipe_text)} characters\")\n",
    "        return recipe_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Generation error: {e}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# Test generation function if model is loaded\n",
    "if MODEL_LOADED:\n",
    "    print(\"üß™ Testing recipe generation on CPU...\")\n",
    "    test_prompt = \"Ingredients: eggs, flour, milk\\nTitle: Simple Pancakes\\nRecipe:\\n\"\n",
    "    \n",
    "    try:\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        test_recipe = generate_recipe_cpu(eval_model, eval_tokenizer, test_prompt, max_length=150)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        generation_time = end_time - start_time\n",
    "        \n",
    "        print(\"‚úÖ Generation function working!\")\n",
    "        print(f\"‚è±Ô∏è Generation time: {generation_time:.1f} seconds\")\n",
    "        print(f\"üìù Sample generation:\")\n",
    "        print(f\"Prompt: {test_prompt.strip()}\")\n",
    "        print(f\"Generated: {test_recipe[:100]}...\")\n",
    "        \n",
    "        # Store the function for later use\n",
    "        generate_recipe = generate_recipe_cpu\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation test failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping generation test - model not loaded\")\n",
    "\n",
    "# Create minimal sample recipes for testing\n",
    "MINIMAL_SAMPLE_PROMPTS = [\n",
    "    \"Ingredients: eggs, flour, milk\\nTitle: Simple Pancakes\\nRecipe:\\n\",\n",
    "    \"Ingredients: chicken, garlic, salt\\nTitle: Garlic Chicken\\nRecipe:\\n\",\n",
    "    \"Ingredients: pasta, tomato, cheese\\nTitle: Simple Pasta\\nRecipe:\\n\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüìù Prepared {len(MINIMAL_SAMPLE_PROMPTS)} minimal test prompts for CPU evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26841e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà MINIMAL EVALUATION METRICS (CPU Optimized)\n",
    "# Calculate basic metrics with minimal computational overhead\n",
    "\n",
    "def calculate_minimal_evaluation(model, tokenizer, num_samples=3):\n",
    "    \"\"\"\n",
    "    Calculate minimal evaluation metrics optimized for CPU\n",
    "    Focus on basic functionality rather than comprehensive analysis\n",
    "    \"\"\"\n",
    "    print(\"üî¨ Starting minimal CPU evaluation...\")\n",
    "    print(f\"üìä Testing with {num_samples} samples for speed\")\n",
    "    \n",
    "    results = {\n",
    "        \"generations\": [],\n",
    "        \"prompts\": [],\n",
    "        \"generation_lengths\": [],\n",
    "        \"generation_times\": [],\n",
    "        \"success_count\": 0\n",
    "    }\n",
    "    \n",
    "    # Use minimal sample prompts\n",
    "    test_prompts = MINIMAL_SAMPLE_PROMPTS[:num_samples]\n",
    "    \n",
    "    print(\"üîÑ Generating test samples...\")\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        try:\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Generate with shorter length for CPU efficiency\n",
    "            generated = generate_recipe_cpu(model, tokenizer, prompt, max_length=100, temperature=0.7)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            generation_time = end_time - start_time\n",
    "            \n",
    "            # Store results\n",
    "            results[\"prompts\"].append(prompt)\n",
    "            results[\"generations\"].append(generated)\n",
    "            results[\"generation_lengths\"].append(len(generated))\n",
    "            results[\"generation_times\"].append(generation_time)\n",
    "            \n",
    "            # Check if generation was successful (not an error message)\n",
    "            if not generated.startswith(\"‚ùå\") and len(generated.strip()) > 10:\n",
    "                results[\"success_count\"] += 1\n",
    "            \n",
    "            print(f\"‚úÖ Sample {i+1}/{num_samples} completed in {generation_time:.1f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing sample {i+1}: {e}\")\n",
    "            results[\"prompts\"].append(prompt)\n",
    "            results[\"generations\"].append(f\"Error: {e}\")\n",
    "            results[\"generation_lengths\"].append(0)\n",
    "            results[\"generation_times\"].append(0)\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    print(\"üî¢ Calculating basic metrics...\")\n",
    "    \n",
    "    successful_lengths = [l for l, g in zip(results[\"generation_lengths\"], results[\"generations\"]) \n",
    "                         if not g.startswith(\"‚ùå\") and l > 10]\n",
    "    successful_times = [t for t, g in zip(results[\"generation_times\"], results[\"generations\"]) \n",
    "                       if not g.startswith(\"‚ùå\") and len(g.strip()) > 10]\n",
    "    \n",
    "    evaluation_report = {\n",
    "        \"total_samples\": len(results[\"generations\"]),\n",
    "        \"successful_generations\": results[\"success_count\"],\n",
    "        \"success_rate\": results[\"success_count\"] / len(results[\"generations\"]) if results[\"generations\"] else 0,\n",
    "        \"avg_generation_length\": np.mean(successful_lengths) if successful_lengths else 0,\n",
    "        \"avg_generation_time\": np.mean(successful_times) if successful_times else 0,\n",
    "        \"total_time\": sum(results[\"generation_times\"]),\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    return evaluation_report\n",
    "\n",
    "# Run minimal evaluation if model is loaded\n",
    "if MODEL_LOADED:\n",
    "    try:\n",
    "        print(\"üöÄ Running minimal evaluation (CPU optimized)...\")\n",
    "        evaluation_results = calculate_minimal_evaluation(eval_model, eval_tokenizer, num_samples=3)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üìã MINIMAL EVALUATION REPORT\")\n",
    "        print(\"=\"*50) \n",
    "        print(f\"üìä Samples tested: {evaluation_results['total_samples']}\")\n",
    "        print(f\"‚úÖ Successful generations: {evaluation_results['successful_generations']}\")\n",
    "        print(f\"üìà Success rate: {evaluation_results['success_rate']:.1%}\")\n",
    "        print(f\"üìè Avg generation length: {evaluation_results['avg_generation_length']:.0f} chars\")\n",
    "        print(f\"‚è±Ô∏è Avg generation time: {evaluation_results['avg_generation_time']:.1f} seconds\")\n",
    "        print(f\"‚è±Ô∏è Total evaluation time: {evaluation_results['total_time']:.1f} seconds\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Show sample outputs\n",
    "        print(\"\\nüìù SAMPLE OUTPUTS:\")\n",
    "        for i, (prompt, generated) in enumerate(zip(evaluation_results['results']['prompts'], \n",
    "                                                   evaluation_results['results']['generations'])):\n",
    "            print(f\"\\nüî∏ Sample {i+1}:\")\n",
    "            print(f\"Prompt: {prompt.strip()}\")\n",
    "            print(f\"Generated: {generated[:150]}...\")\n",
    "        \n",
    "        EVALUATION_COMPLETED = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Minimal evaluation failed: {e}\")\n",
    "        EVALUATION_COMPLETED = False\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping evaluation - model not loaded\")\n",
    "    EVALUATION_COMPLETED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® MINIMAL GENERATION SHOWCASE (CPU Optimized)\n",
    "# Generate sample recipes with minimal computational overhead\n",
    "\n",
    "def minimal_showcase_generation():\n",
    "    \"\"\"Generate minimal sample recipes optimized for CPU\"\"\"\n",
    "    \n",
    "    print(\"üé® MINIMAL RECIPE SHOWCASE (CPU Optimized)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Reduced showcase prompts\n",
    "    showcase_prompts = [\n",
    "        {\n",
    "            \"prompt\": \"Ingredients: chicken, garlic, herbs\\nTitle: Herb Chicken\\nRecipe:\\n\",\n",
    "            \"description\": \"üçó Simple Chicken Dish\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Ingredients: eggs, flour, sugar\\nTitle: Basic Cake\\nRecipe:\\n\", \n",
    "            \"description\": \"\udf70 Easy Dessert\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Simplified creativity levels\n",
    "    creativity_levels = [\n",
    "        {\"temp\": 0.6, \"top_p\": 0.8, \"name\": \"Conservative\"},\n",
    "        {\"temp\": 0.9, \"top_p\": 0.9, \"name\": \"Creative\"}\n",
    "    ]\n",
    "    \n",
    "    showcase_results = []\n",
    "    \n",
    "    for prompt_info in showcase_prompts:\n",
    "        print(f\"\\n{prompt_info['description']}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for creativity in creativity_levels:\n",
    "            try:\n",
    "                import time\n",
    "                start_time = time.time()\n",
    "                \n",
    "                recipe = generate_recipe_cpu(\n",
    "                    eval_model, \n",
    "                    eval_tokenizer, \n",
    "                    prompt_info['prompt'],\n",
    "                    max_length=120,  # Shorter for CPU\n",
    "                    temperature=creativity['temp'],\n",
    "                    top_p=creativity['top_p']\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                \n",
    "                print(f\"üìù {creativity['name']} ({end_time-start_time:.1f}s):\")\n",
    "                print(f\"   {recipe[:100]}...\")\n",
    "                print()\n",
    "                \n",
    "                showcase_results.append({\n",
    "                    \"prompt\": prompt_info['prompt'],  \n",
    "                    \"creativity_level\": creativity['name'],\n",
    "                    \"generated\": recipe,\n",
    "                    \"generation_time\": end_time - start_time\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Generation failed for {creativity['name']}: {e}\")\n",
    "    \n",
    "    return showcase_results\n",
    "\n",
    "# Generate minimal showcase if model is loaded\n",
    "if MODEL_LOADED:\n",
    "    try:\n",
    "        showcase_data = minimal_showcase_generation()\n",
    "        print(f\"\\n‚úÖ Generated {len(showcase_data)} showcase examples\")\n",
    "        \n",
    "        # Calculate average generation time\n",
    "        avg_time = np.mean([item['generation_time'] for item in showcase_data if 'generation_time' in item])\n",
    "        print(f\"‚è±Ô∏è Average generation time: {avg_time:.1f} seconds\")\n",
    "        \n",
    "        SHOWCASE_COMPLETED = True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Showcase generation failed: {e}\")\n",
    "        SHOWCASE_COMPLETED = False\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping showcase - model not loaded\")\n",
    "    SHOWCASE_COMPLETED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e74180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä FINAL EVALUATION REPORT (CPU Optimized)\n",
    "# Create a concise final report optimized for local CPU evaluation\n",
    "\n",
    "def create_minimal_evaluation_report():\n",
    "    \"\"\"Create a concise evaluation report for CPU-based evaluation\"\"\"\n",
    "    \n",
    "    print(\"üìä FINAL EVALUATION REPORT (CPU Mode)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Model Performance Summary  \n",
    "    print(\"ü§ñ MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if MODEL_LOADED:\n",
    "        try:\n",
    "            total_params = sum(p.numel() for p in eval_model.parameters())\n",
    "            print(f\"üìà Model: GPT-2 ({model_type})\")\n",
    "            print(f\"üìà Total Parameters: {total_params:,}\")\n",
    "            print(f\"üìà Device: {device}\")\n",
    "            print(f\"üìà Status: ‚úÖ Loaded Successfully\")\n",
    "        except:\n",
    "            print(f\"üìà Model: Loaded but parameter count unavailable\")\n",
    "            print(f\"üìà Type: {model_type if 'model_type' in globals() else 'Unknown'}\")\n",
    "            print(f\"üìà Device: {device}\")\n",
    "    else:\n",
    "        print(f\"üìà Status: ‚ùå Not Loaded\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Evaluation Results Summary\n",
    "    print(\"üìä EVALUATION RESULTS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if EVALUATION_COMPLETED and 'evaluation_results' in globals():\n",
    "        results = evaluation_results\n",
    "        print(f\"üìè Samples Tested: {results['total_samples']}\")\n",
    "        print(f\"‚úÖ Success Rate: {results['success_rate']:.1%}\")\n",
    "        print(f\"üìè Avg Length: {results['avg_generation_length']:.0f} chars\")\n",
    "        print(f\"‚è±Ô∏è Avg Time: {results['avg_generation_time']:.1f} seconds\")\n",
    "        print(f\"‚è±Ô∏è Total Time: {results['total_time']:.1f} seconds\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        if results['avg_generation_time'] < 5:\n",
    "            print(\"\ude80 Performance: Good (< 5s per generation)\")\n",
    "        elif results['avg_generation_time'] < 15:\n",
    "            print(\"‚ö†Ô∏è Performance: Acceptable (5-15s per generation)\")\n",
    "        else:\n",
    "            print(\"üêå Performance: Slow (> 15s per generation)\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Evaluation metrics not available\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Generation Quality\n",
    "    print(\"üé® GENERATION QUALITY\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if SHOWCASE_COMPLETED and 'showcase_data' in globals():\n",
    "        print(f\"‚úÖ Showcase: {len(showcase_data)} examples generated\")\n",
    "        \n",
    "        # Analyze generation quality\n",
    "        avg_lengths = [len(item['generated']) for item in showcase_data if 'generated' in item]\n",
    "        if avg_lengths:\n",
    "            print(f\"üìè Avg Recipe Length: {np.mean(avg_lengths):.0f} characters\")\n",
    "            \n",
    "        # Check for quality indicators\n",
    "        quality_indicators = 0\n",
    "        for item in showcase_data:\n",
    "            if 'generated' in item and len(item['generated']) > 50:\n",
    "                quality_indicators += 1\n",
    "                \n",
    "        quality_score = quality_indicators / len(showcase_data) if showcase_data else 0\n",
    "        print(f\"‚úÖ Quality Score: {quality_score:.1%} (recipes > 50 chars)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Generation showcase not available\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # System Assessment\n",
    "    print(\"üèÜ SYSTEM ASSESSMENT\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    assessment_score = 0\n",
    "    max_score = 4\n",
    "    \n",
    "    if MODEL_LOADED:\n",
    "        assessment_score += 1\n",
    "        print(\"‚úÖ Model Loading: SUCCESS\")\n",
    "    else:\n",
    "        print(\"‚ùå Model Loading: FAILED\")\n",
    "    \n",
    "    if EVALUATION_COMPLETED:\n",
    "        assessment_score += 1\n",
    "        print(\"‚úÖ Basic Evaluation: COMPLETED\")\n",
    "    else:\n",
    "        print(\"‚ùå Basic Evaluation: FAILED\")\n",
    "        \n",
    "    if SHOWCASE_COMPLETED:\n",
    "        assessment_score += 1\n",
    "        print(\"‚úÖ Generation Test: COMPLETED\")\n",
    "    else:\n",
    "        print(\"‚ùå Generation Test: FAILED\")\n",
    "    \n",
    "    # Check if it's actually generating reasonable content\n",
    "    if ('evaluation_results' in globals() and \n",
    "        evaluation_results.get('success_rate', 0) > 0.5):\n",
    "        assessment_score += 1\n",
    "        print(\"‚úÖ Generation Quality: ACCEPTABLE\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Generation Quality: NEEDS IMPROVEMENT\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"üéØ Overall Score: {assessment_score}/{max_score} ({100*assessment_score/max_score:.0f}%)\")\n",
    "    \n",
    "    if assessment_score >= 3:\n",
    "        print(\"üéâ GOOD: System is working well for local CPU evaluation!\")\n",
    "        print(\"üí° Tip: Generation is slower on CPU but functional\")\n",
    "    elif assessment_score >= 2:\n",
    "        print(\"üëç FAIR: Basic functionality working\")\n",
    "        print(\"üí° Tip: Some features may need attention\")\n",
    "    else:\n",
    "        print(\"‚ùå POOR: System needs significant fixes\")\n",
    "        print(\"üîß Check model files and dependencies\")\n",
    "    \n",
    "    # Performance recommendations\n",
    "    print(\"\\nüí° CPU OPTIMIZATION TIPS:\")\n",
    "    print(\"‚Ä¢ Reduce max_length for faster generation\")\n",
    "    print(\"‚Ä¢ Use temperature 0.7-0.8 for good quality/speed balance\")\n",
    "    print(\"‚Ä¢ Consider batch processing for multiple recipes\")\n",
    "    print(\"‚Ä¢ GPU would significantly improve performance\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã CPU EVALUATION COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Generate final report\n",
    "create_minimal_evaluation_report()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2957522,
     "sourceId": 5093016,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
